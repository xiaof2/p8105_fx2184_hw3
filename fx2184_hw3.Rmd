---
title: "p8105_fx2184_hw3"
author: "Fei"
date: "2022-10-16"
output: github_document
---


```{r}
library(tidyverse)
library(readr)
library(devtools)
library(patchwork)
library(ggridges)
```

# problem1 

```{r}
#import the dataset
library(p8105.datasets)
instacart_df = instacart  %>% 
        as_tibble(instacart)
# giving illstrative examples of first 10 observations
head(instacart_df, 10)
``` 
* the instacart dataset has `r nrow(instacart_df)` observations, and `r ncol(instacart_df)` variables, they are `r names(instacart_df)`. 
* In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

```{r warning=FALSE}
# How many aisles are there, and which aisles are the most items ordered from?
n_aisles= instacart_df %>%
  group_by(aisle_id,aisle) %>% 
  distinct() %>% 
  summarize(n_order = n()) %>% 
  arrange(desc(n_order)) %>% 
  ungroup()

# Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.
n_aisles %>%
  filter(n_order >10000) %>% 
  ggplot(aes(x =n_order, y=aisle, fill = aisle))+
  geom_bar(
    stat = "identity"
  )  +
  labs(
    title = 'Number of Items Ordered in Each Aisle',
    caption = 'Aisles with more than 10000 items ordered',
    x = ' Number of items',
    y = ' Aisle'
  ) + theme(legend.position = "topright") 
```
* There are `r nrow(n_aisles)` aisles are there, and the most ites ordered from `fresh vegetables`. 
* The plot shows the number of items ordered in each aisle. 

```{r warning=FALSE}
# Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

instacart_df %>% 
  filter(aisle == c("dog food care", "packaged vegetables fruits","baking ingredients")) %>%              
  group_by(aisle, product_name) %>% 
  summarize(n_order = n()) %>% 
  arrange(desc(n_order)) %>% 
  do(head(.,n =3)) %>% 
  knitr::kable(
    caption = "Three Most Popular Items in each of the aisles")
```

```{r warning=FALSE}
# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
instacart_df %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

# problem 2 

Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?
Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r warning=FALSE}
# tidy the dataset 
accel<- read_csv("accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "minute",
    values_to = "activity",
    names_prefix = "activity_", 
  ) %>% 
  mutate(
    day_of_week = recode(day,
                         "Monday" = "Weekday", 
                         "Tuesday" = "Weekday", 
                         "Wednesday" = "Weekday", 
                         "Thursday" = "Weekday", 
                         "Friday" = "Weekday", 
                         "Saturday" = "Weekend", 
                         "Sunday" = "Weekend"))
```
* The dataset `accel` contains `r nrow(accel)` observations, and  `r ncol(accel)` varibles, which are `r names(accel)`. 
* Each observation represent a male subject's activity count per minute of each day during five weeks.

#aggregate across minutes 
```{r warning=FALSE}
#aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?
accel %>% 
  group_by(week,day) %>% 
  summarize(total_activity=sum(activity)) %>% 
  spread(key= day, total_activity) %>% 
  select("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday") %>% 
  knitr::kable(digits = 0,
    catpion = "Aggregation of a Total Activity across Minutes for Each Day")
```
* From the table, it hard to tell there is a trend exists. However, in the first 3 weeks, it seems like the subject has higher total activity during the weekend than weekdays. 

```{r}
#Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.
accel %>% 
  group_by(minute,day) %>% 
  mutate(minute =as.numeric(minute)) %>% 
  ggplot(aes(x = minute/60, y = activity)) + 
  geom_point(aes(color = day), alpha = .5) + 
   scale_x_continuous(
    breaks = c(0, 3, 6, 9, 12, 15, 18, 21, 24),
    labels = c("0hr", "3hr", "6hr", "9hr", "12hr", "15hr", "18hr", "21hr", "24hr")) + 
  labs(
    title = "24-hour Activity Time Courses for Each Day with one-minute interval",
    x = "Hours",
    y = "Activity",
    caption = "single_panel plot")+
  theme(plot.title = element_text(size=12))
```

* Between 0hr (12am) to 6hr (6am), it is to be observed that the activity count of a 63 year-old male is the lowest across days. Around 7hr (7am), his activity count is relatively high on some Thursdays. Around 9hr (9am), his activity count is quite high on some Fridays. Around 12hr (12pm), his activity count is high on many Sundays. Between 16hr (4pm) and 17hr (5pm), his activity count is relatively high on a decent number of weekends. Between 20hr (8pm) and 22hr(10pm), his activity count is high across many days, especially on Friday followed by Saturday, Wednesday, Monday. 



# problem 3 
```{r warning=FALSE}
library(p8105.datasets)
data("ny_noaa")
# clean and correct unit 
ny_noaa = 
  ny_noaa %>% 
  as_tibble(ny_noaa) %>% 
    janitor::clean_names()%>% 
    mutate(data = as.Date(date)) %>% 
    mutate(year = as.integer(format(date, format = "%Y")),
         month = as.integer(format(date, format = "%m")),
         day = as.integer(format(date, format = "%d"))) %>% 
    mutate(tmax = as.numeric(tmax)/10) %>% 
    mutate(prcp = as.numeric(prcp)/10 ) %>% 
    mutate(tmin = as.numeric(tmin)/10 ) 
```

# the most commonly observed values of snowfall
```{r}
ny_noaa %>% 
  group_by(snow) %>% 
  distinct() %>% 
  summarize(freq = n()) %>% 
  arrange(desc(freq)) %>% 
  head(3) %>% 
  knitr::kable()
```
* The most commonly observed values of snowfall is `0` mm, which happened `2008508` times in the new york city. 
* During the snowing day, the most frequent observed values of snowfall is `25`mm, which happened `31022` times. ]

# Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r}
 ny_noaa %>% 
   filter(month == 1| month == 7) %>% 
   mutate(month = month.abb[month]) %>% 
   filter(!is.na(tmax)) %>% 
   mutate(tmax = as.numeric(tmax)) %>% 
   group_by(month,year,id) %>% 
   summarise(mean_month = mean(tmax)) %>% 
   ggplot(aes(
     x=year,
     y=mean_month,
     color=year))+
   geom_point() +
  facet_grid(.~month)+
  labs(x = "Year",
       y = "Average Max Temperature (C)",
       caption = "Each point represents the average max temperature of a station at the given time")+
  ggtitle('The mean max temperature in January and July in each station across years')+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

* The two-panel plot shows the average max temperature in January and in July in each station across years among 1981 to 2010. The July has an overall higher average max temperature than January, which is reasonable because new york city usually has distinctive four seasons. Outliers are exists both in January and July due to extreme weather. 

# Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option)

```{r}
plot1= ny_noaa %>%
        filter(!is.na(tmax),!is.na(tmin)) %>%
        ggplot(aes(x = tmin, y = tmax)) +
        geom_hex() + 
        labs(x = "Minimum Temperature (C)", 
             y = "Maximum Temperature (C)",
             title = "Max vs Min Temperature") +
        theme(plot.title = element_text(size = 12)) +
        theme(axis.title = element_text(size = 10)) +
        theme(axis.text = element_text(size = 10)) 
plot1
```

```{r}
# make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
plot2 = ny_noaa %>% 
  filter(!is.na(snow)) %>%
  filter(snow>0,snow<100) %>%
  ggplot(aes(x=snow, y=as.factor(year)))+
  geom_density_ridges(alpha=0.8)+
  ggtitle('The distribution of snowfall values between 0 and 100 by year')+
  theme(legend.position = "bottom")
plot2
```

#patch work 
```{r}
plot1+plot2
```

* The hex plot shows that the high frequencies concentrate in the middle, which indicates that when the min temp within 0-20 degree Celcius and max temp within 0-25 degree celcius, we have the most of observations. 

* The snowfall density curve indicates that the density curves overlap a lot, which indicates that the snowfall from 1981 to 2004 does not change greatly over the years. The most frequent snowfall value is 25mm in range 0 to 100mm. 

